# NLP Classifier - Hybrid Architecture

Este proyecto implementa un sistema de clasificaciÃ³n de texto hÃ­brido que combina la velocidad de un modelo Transformer local con la precisiÃ³n de un LLM (Large Language Model).

## ðŸ§  Â¿CÃ³mo funciona?

El sistema utiliza una arquitectura **HÃ­brida (Hybrid Classifier)** diseÃ±ada para optimizar costos y precisiÃ³n:

1.  **Fast Path (Modelo Local)**: El texto ingresa primero a un modelo Transformer (basado en BERT/RoBERTa) entrenado localmente.
    *   âœ… **Ventaja**: Extremadamente rÃ¡pido (~10ms) y muy barato.
    *   âŒ **Desventaja**: Puede fallar en casos complejos o ambiguos.
2.  **Confidence Check**: El sistema evalÃºa la "confianza" de la predicciÃ³n del modelo local.
    *   Si `confianza > umbral` (ej. 0.75): Se devuelve la respuesta del modelo local.
    *   Si `confianza < umbral`: Se activa el "Slow Path".
3.  **Slow Path (Fallback a LLM)**: Se envÃ­a el texto a un LLM (ej. GPT-4, Claude) para que lo clasifique.
    *   âœ… **Ventaja**: Alta capacidad de razonamiento y comprensiÃ³n de contexto.
    *   âŒ **Desventaja**: MÃ¡s lento y costoso.

Esta arquitectura permite que el 90% de las peticiones sean resueltas por el modelo rÃ¡pido (gratis), usando el LLM solo para los casos difÃ­ciles.

---

## ðŸ“‚ Estructura del Proyecto

### `packages/classifier_core/`
Este es el "cerebro" del proyecto. Es una librerÃ­a de Python instalable que contiene toda la lÃ³gica de negocio.

*   `hybrid_classifier.py`: **Core del sistema**. Contiene la clase `HybridClassifier` que implementa la lÃ³gica de decisiÃ³n explicada arriba (Modelo vs LLM).
*   `model.py`: Define la clase `TextClassifier` que envuelve el modelo Transformer (Hugging Face) para realizar predicciones.
*   `llm_client.py`: Gestiona la conexiÃ³n con proveedores de LLM (OpenAI, Anthropic).
*   `data.py`: Funciones para cargar y preprocesar los datos de texto.
*   `train.py`: LÃ³gica de entrenamiento basada en PyTorch/Transformers.

### `services/api/`
Es la interfaz externa del sistema. Expone la funcionalidad a travÃ©s de una API REST usando **FastAPI**.

*   `main.py`: Punto de entrada de la API. Define los endpoints:
    *   `POST /predict`: Usa solo el modelo local.
    *   `POST /predict/hybrid`: Usa la lÃ³gica hÃ­brida.
    *   `GET /stats`: Muestra mÃ©tricas de uso (cuÃ¡ntas veces se usÃ³ el LLM, costos ahorrados, etc.).
*   `schemas.py`: Define los modelos de datos (Pydantic) para las peticiones y respuestas (ej. quÃ© formato tiene el JSON de entrada).
*   `deps.py`: InyecciÃ³n de dependencias para cargar el modelo una sola vez al iniciar la API.

### `scripts/`
Scripts ejecutables para el ciclo de vida de Machine Learning (MLOps).

*   `prepare_data.py`: Limpia y prepara los datasets crudos.
*   `train.py`: Entrena un nuevo modelo Transformer y lo guarda en `models/`.
*   `evaluate.py`: Mide la precisiÃ³n del modelo en el set de prueba.

### `models/` y `mlruns/`
*   `models/`: Almacena los artefactos de los modelos entrenados (pesos, tokenizadores).
*   `mlruns/`: Directorio de **MLflow** para traquear experimentos (mÃ©tricas de entrenamiento, hiperparÃ¡metros).

### `docker-compose.yml`
Orquesta los servicios para levantar todo el entorno con un solo comando:
1.  **api**: Levanta el servidor FastAPI en el puerto 8000.
2.  **mlflow**: Levanta la interfaz de MLflow en el puerto 5000 para visualizar experimentos.

---

## ðŸš€ GuÃ­a de Inicio

### 1. Requisitos
*   Docker y Docker Compose
*   (Opcional) Python 3.11 para desarrollo local

### 2. Ejecutar con Docker
Para levantar la API y MLflow:

```bash
docker-compose up --build
```

La API estarÃ¡ disponible en `http://localhost:8000/docs` (Swagger UI).

### 3. Desarrollo Local
Si quieres entrenar o modificar el cÃ³digo:

```bash
# Instalar dependencias
pip install -e .[dev]

# Entrenar modelo
python scripts/train.py

# Ejecutar API localmente
uvicorn services.api.main:app --reload
```

## ðŸ“Š MÃ©tricas y Monitoreo
El sistema rastrea automÃ¡ticamente:
*   **Model Ratio**: Porcentaje de peticiones resueltas por el modelo local.
*   **LLM Ratio**: Porcentaje de peticiones que requirieron LLM.
*   **Ahorro de Costos**: ComparaciÃ³n estimada vs usar LLM para todo.
*   **Latencia**: Tiempo de respuesta promedio.

Endpoints Ãºtiles:
*   `GET /health`: Ver estado del sistema.
*   `GET /stats`: Ver mÃ©tricas de rendimiento.
